{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6ae7d5",
   "metadata": {},
   "source": [
    "# Input dataset source and database credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabebd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, boxcox\n",
    "\n",
    "import sqlite3\n",
    "import psycopg2\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import create_engine, inspect, MetaData, Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbf8f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputer_func():\n",
    "    \"\"\"\n",
    "    Prompt the user to input necessary information for data extraction and transformation.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the folder path where the dataset is located,\n",
    "             source database connection details (username, password, server, database name),\n",
    "             and destination database connection details (username, password, server, database name).\n",
    "    \"\"\"\n",
    "    # Dataset folderpath\n",
    "    folder_path = input('Enter the folder path (without quotes): ')\n",
    "\n",
    "    # Source database\n",
    "    while True:\n",
    "        try:\n",
    "            SOURCE_USERNAME = input('Enter Source Username: ')\n",
    "            SOURCE_PASSWORD = input('Enter Source Password: ')\n",
    "            SOURCE_SERVER = input('Enter Source Server: ')\n",
    "            SOURCE_DATABASE = input('Enter Source Database: ')\n",
    "            # Check if all inputs are provided\n",
    "            if all([SOURCE_USERNAME, SOURCE_PASSWORD, SOURCE_SERVER, SOURCE_DATABASE]):\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please provide all required information.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    # Destination database\n",
    "    while True:\n",
    "        try:\n",
    "            DEST_USERNAME = input('Enter Destination Username: ')\n",
    "            DEST_PASSWORD = input('Enter Destination Password: ')\n",
    "            DEST_SERVER = input('Enter Destination Server: ')\n",
    "            DEST_DATABASE = input('Enter Destination Database: ')\n",
    "            # Check if all inputs are provided\n",
    "            if all([DEST_USERNAME, DEST_PASSWORD, DEST_SERVER, DEST_DATABASE]):\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please provide all required information.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    return (folder_path, SOURCE_USERNAME, SOURCE_PASSWORD, SOURCE_SERVER, SOURCE_DATABASE,\n",
    "            DEST_USERNAME, DEST_PASSWORD, DEST_SERVER, DEST_DATABASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9972c1c",
   "metadata": {},
   "source": [
    "EXTRACT & LOAD RAW DATA TO DATABASE (STAGING PHASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45456687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_data_func(folder_path):\n",
    "    \"\"\"\n",
    "    Extract CSV data from the specified folder.\n",
    "    \n",
    "    Args:\n",
    "    - folder_path (str): Path to the folder containing CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    - data_frames (list): List of pandas DataFrames containing data from each of the CSV files.\n",
    "    \"\"\"\n",
    "    data_frames = []\n",
    "    \n",
    "    # Check if the folder path exists\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: Folder path '{folder_path}' does not exist.\")\n",
    "        return data_frames\n",
    "    \n",
    "    # Iterate over files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                # Read CSV file into a DataFrame\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Add a column to store the file name\n",
    "                df['file_name'] = file_name\n",
    "                # Append the DataFrame to the list\n",
    "                data_frames.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file '{file_name}': {e}\")\n",
    "    \n",
    "    if not data_frames:\n",
    "        print(\"No CSV files found in the specified folder.\")\n",
    "    \n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8330dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def staging_func(data_frames, SOURCE_USERNAME, SOURCE_PASSWORD, SOURCE_SERVER, SOURCE_DATABASE):\n",
    "    \"\"\"\n",
    "    Stages extracted data for transformation.\n",
    "\n",
    "    Args:\n",
    "    - data_frames (list): List of DataFrames containing extracted data.\n",
    "    - SOURCE_USERNAME (str): Username for the source database connection.\n",
    "    - SOURCE_PASSWORD (str): Password for the source database connection.\n",
    "    - SOURCE_SERVER (str): Server address for the source database connection.\n",
    "    - SOURCE_DATABASE (str): Name of the source database.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Timer for staging data for transformation\n",
    "        start_time = time.time()\n",
    "        print(f\"Staging data for transformation...{start_time}\")\n",
    "        \n",
    "        # Connect to the source database\n",
    "        source_conn_str = f'postgresql://{SOURCE_USERNAME}:{SOURCE_PASSWORD}@{SOURCE_SERVER}/{SOURCE_DATABASE}'\n",
    "        source_engine = create_engine(source_conn_str)\n",
    "        \n",
    "        # Create a session to handle transactions\n",
    "        Session = sessionmaker(bind=source_engine)\n",
    "        session = Session()\n",
    "        \n",
    "        # Iterate over data frames and stage data\n",
    "        for df in data_frames:\n",
    "            # Add new columns 'fileimportedby' and 'fileimportdatetime' to each DataFrame\n",
    "            df['fileimportedby'] = SOURCE_USERNAME\n",
    "            df['fileimportdatetime'] = datetime.now()\n",
    "\n",
    "            table_name = df['file_name'].iloc[0].split('.')[0]  # Extract table name from file name\n",
    "            inspector = inspect(source_engine)\n",
    "            if not inspector.has_table(table_name):\n",
    "                df.head(0).to_sql(table_name, source_engine, index=False)  # Create table if it doesn't exist\n",
    "            \n",
    "            df.to_sql(table_name, source_engine, if_exists='append', index=False)  # Append data to table\n",
    "        \n",
    "        # Calculate time taken for staging\n",
    "        staging_time = time.time() - start_time\n",
    "        print(f\"Data successfully staged. Time taken: {staging_time} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during staging: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e2e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction_func(SOURCE_USERNAME, SOURCE_PASSWORD, SOURCE_SERVER, SOURCE_DATABASE):\n",
    "    \"\"\"\n",
    "    Extracts data from the source database.\n",
    "\n",
    "    Args:\n",
    "    - SOURCE_USERNAME (str): Username for the source database connection.\n",
    "    - SOURCE_PASSWORD (str): Password for the source database connection.\n",
    "    - SOURCE_SERVER (str): Server address for the source database connection.\n",
    "    - SOURCE_DATABASE (str): Name of the source database.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of DataFrames containing extracted data from the source database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Timer for loading data from source database\n",
    "        start_time = time.time()\n",
    "        print(f\"Loading data from source database...{start_time} seconds\")\n",
    "        \n",
    "        # Connect to the source database\n",
    "        source_conn_str = f'postgresql://{SOURCE_USERNAME}:{SOURCE_PASSWORD}@{SOURCE_SERVER}/{SOURCE_DATABASE}'\n",
    "        source_engine = create_engine(source_conn_str)\n",
    "        \n",
    "        # Timer for extracting data for transformation\n",
    "        start_time = time.time()\n",
    "        print(f\"Extracting data for transformation...{start_time} seconds\")\n",
    "        \n",
    "        # Extract data for transformation\n",
    "        metadata = MetaData()\n",
    "        metadata.reflect(bind=source_engine)\n",
    "\n",
    "        # Create a session to handle transactions\n",
    "        Session = sessionmaker(bind=source_engine)\n",
    "        session = Session()\n",
    "\n",
    "        extracted_data = []\n",
    "        for table_name, table in metadata.tables.items():\n",
    "            query = table.select()\n",
    "            result = session.execute(query).fetchall()\n",
    "            df = pd.DataFrame(result, columns=table.columns.keys())\n",
    "            extracted_data.append(df)\n",
    "        \n",
    "        # Calculate time taken for extraction\n",
    "        extraction_time = time.time() - start_time\n",
    "        print(f\"Data successfully extracted. Time taken: {extraction_time} seconds\")\n",
    "              \n",
    "        return extracted_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data extraction: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81339431",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b8705",
   "metadata": {},
   "source": [
    "<b>1. Handle Duplicates:</b>\n",
    "\n",
    "Are there duplicate rows in the dataset that need to be removed?\n",
    "\n",
    "How will the removal of duplicates impact the analysis or modeling tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b73c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_duplicates(data_frames):\n",
    "    \"\"\"\n",
    "    Remove duplicates from each DataFrame in the list.\n",
    "\n",
    "    Args:\n",
    "    - data_frames (list): List of DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of DataFrames with duplicates removed.\n",
    "    \"\"\"\n",
    "    return [df.drop_duplicates() for df in data_frames]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a315a",
   "metadata": {},
   "source": [
    "<b>2. Handle Missing Values:</b>\n",
    "\n",
    "Are there columns with over 90% missing values and rows with 100% missing values in the dataset?\n",
    "\n",
    "How will the removal of duplicates impact the analysis or modeling tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a4c723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(data_frames, column_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Handle missing values in a list of DataFrames by dropping columns with over 90% missing values\n",
    "    and rows that are 100% null.\n",
    "\n",
    "    Args:\n",
    "        data_frames (list): List of pandas DataFrames.\n",
    "        column_threshold (float): Threshold for dropping columns based on the proportion of missing values.\n",
    "\n",
    "    Returns:\n",
    "        cleaned_data_frames (list): List of DataFrames with missing values handled.\n",
    "    \"\"\"\n",
    "    cleaned_data_frames = []\n",
    "\n",
    "    for df in data_frames:\n",
    "        # Drop columns with over 90% missing values\n",
    "        df = df.dropna(axis=1, thresh=int(column_threshold * len(df)))\n",
    "        \n",
    "        # Drop rows that are 100% null\n",
    "        df = df.dropna(axis=0, how='all')\n",
    "        \n",
    "        # Reset index\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        cleaned_data_frames.append(df)\n",
    "\n",
    "    return cleaned_data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9272a8",
   "metadata": {},
   "source": [
    "<b>3. Data Type:</b> \n",
    "\n",
    "What are the data types of the values? (Numeric, categorical, or other?)\n",
    "\n",
    "If numeric, are they continuous or discrete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a0a872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_datatypes(data_frames):\n",
    "    \"\"\"\n",
    "    Classify columns in a list of DataFrames into different data types: numeric, datetime, date, and categorical.\n",
    "\n",
    "    Parameters:\n",
    "        dataframes (list): A list of pandas DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        numeric_columns_all (list): List of columns classified as numeric across all DataFrames.\n",
    "        datetime_columns_all (list): List of columns classified as datetime across all DataFrames.\n",
    "        date_columns_all (list): List of columns classified as date across all DataFrames.\n",
    "        categorical_columns_all (list): List of columns classified as categorical across all DataFrames.\n",
    "    \"\"\"\n",
    "    numeric_columns_all = []\n",
    "    datetime_columns_all = []\n",
    "    date_columns_all = []\n",
    "    categorical_columns_all = []\n",
    "    data_frames = data_frames.copy()\n",
    "\n",
    "    for df in data_frames:\n",
    "        numeric_columns, datetime_columns, date_columns, categorical_columns = [], [], [], []\n",
    "\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype in ['float64', 'int64']:\n",
    "                numeric_columns.append(col)\n",
    "            elif df[col].astype(str).str.match(r'\\d+/\\d+/\\d+ \\d+:\\d+:\\d+ [AP]M').all():\n",
    "                datetime_columns.append(col)\n",
    "            elif df[col].astype(str).str.match(r'\\d+/\\d+/\\d+').all():\n",
    "                date_columns.append(col)\n",
    "            else:\n",
    "                categorical_columns.append(col)\n",
    "\n",
    "        numeric_columns_all.extend(numeric_columns)\n",
    "        datetime_columns_all.extend(datetime_columns)\n",
    "        date_columns_all.extend(date_columns)\n",
    "        categorical_columns_all.extend(categorical_columns)\n",
    "\n",
    "    return numeric_columns_all, datetime_columns_all, date_columns_all, categorical_columns_all, data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bf5c4",
   "metadata": {},
   "source": [
    "<b>4. Data Context:</b>\n",
    "\n",
    "What do the values represent? (Measurements, counts, percentages, etc.)\n",
    "\n",
    "What is the context or domain of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886e684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_numeric_columns(data_frames, numeric_columns_all, threshold=0.06):\n",
    "    \"\"\"\n",
    "    Classify numeric columns in a list of DataFrames into count columns and measurement columns.\n",
    "    \n",
    "    Parameters:\n",
    "        data_frames (list): List of DataFrames.\n",
    "        numeric_columns_all (list): List of all numeric columns across all DataFrames.\n",
    "        threshold (float): Threshold to differentiate count columns from measurement columns based on the proportion of unique values.\n",
    "                           Columns with a proportion of unique values less than or equal to the threshold are classified as count columns.\n",
    "    \n",
    "    Returns:\n",
    "        count_columns_all (list): List of all count columns across all DataFrames.\n",
    "        measurement_columns_all (list): List of all measurement columns across all DataFrames.\n",
    "        data_frames (list): A copy of the original list of DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    count_columns_all = []\n",
    "    measurement_columns_all = []\n",
    "    data_frames = data_frames.copy()\n",
    "\n",
    "    for df in data_frames:\n",
    "        count_columns = []\n",
    "        measurement_columns = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col in numeric_columns_all: # Check if column exists in list of numeric columns\n",
    "                unique_ratio = df[col].nunique() / len(df[col])\n",
    "                \n",
    "                if df[col].astype(str).str.match(r'0\\..*').any():\n",
    "                    measurement_columns.append(col)\n",
    "                elif unique_ratio <= threshold or col.endswith('Id'):\n",
    "                    count_columns.append(col)\n",
    "                else:\n",
    "                    measurement_columns.append(col)\n",
    "        \n",
    "        count_columns_all.extend(count_columns)\n",
    "        measurement_columns_all.extend(measurement_columns)\n",
    "\n",
    "    return count_columns_all, measurement_columns_all, data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf527e",
   "metadata": {},
   "source": [
    "<b>5. Handle Outliers:</b> \n",
    "    \n",
    "Are the values within a reasonable range for the context of the project?\n",
    "\n",
    "Are there any outliers that might skew the analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34ded5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detector_and_decider_func(data_frames, measurement_columns_all, significance_threshold=0, removal_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Detects outliers in the given DataFrames and decides on the approach to handle them.\n",
    "\n",
    "    Parameters:\n",
    "        data_frames (list): List of pandas DataFrames containing the data.\n",
    "        measurement_columns_all (list): List of all columns considered as measurement columns.\n",
    "        significance_threshold (float): Threshold for considering a difference significant in outlier detection.\n",
    "        removal_threshold (float): Threshold for determining whether to remove outliers or adjust their values.\n",
    "\n",
    "    Returns:\n",
    "        outlier_info (dict): Dictionary containing information about outliers and their handling approach for each column.\n",
    "                             Keys: column name, Values: {'is_outlier': bool, 'correction_approach': str}.\n",
    "                             'is_outlier': True if the column contains outliers, False otherwise.\n",
    "                             'correction_approach': Approach to handle outliers ('remove_outliers' or 'adjust_values').\n",
    "    \"\"\"\n",
    "    outlier_info = {}\n",
    "\n",
    "    def detect_outliers(df, col):\n",
    "        \"\"\"\n",
    "        Detects outliers in a column based on the given DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing the data.\n",
    "            col (str): Column name to detect outliers.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if outliers are detected, False otherwise.\n",
    "        \"\"\"\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        return not outliers.empty\n",
    "\n",
    "    def decide_outlier_correction(df_with_outliers, df_without_outliers, col):\n",
    "        \"\"\"\n",
    "        Decides on the approach to handle outliers in a column.\n",
    "\n",
    "        Args:\n",
    "            df_with_outliers (pd.DataFrame): DataFrame containing outliers.\n",
    "            df_without_outliers (pd.DataFrame): DataFrame without outliers.\n",
    "            col (str): Column name to handle outliers.\n",
    "\n",
    "        Returns:\n",
    "            dict: Information about outlier detection and handling approach.\n",
    "                  {'is_outlier': bool, 'correction_approach': str}.\n",
    "                  'is_outlier': True if the column contains outliers, False otherwise.\n",
    "                  'correction_approach': Approach to handle outliers ('remove_outliers' or 'adjust_values').\n",
    "        \"\"\"\n",
    "        column_data_with_outliers = df_with_outliers[col]\n",
    "        column_data_without_outliers = df_without_outliers[col]\n",
    "\n",
    "        mean_with_outliers = column_data_with_outliers.mean()\n",
    "        median_with_outliers = column_data_with_outliers.median()\n",
    "        mean_without_outliers = column_data_without_outliers.mean()\n",
    "        median_without_outliers = column_data_without_outliers.median()\n",
    "\n",
    "        mean_difference = mean_with_outliers - mean_without_outliers\n",
    "        median_difference = median_with_outliers - median_without_outliers\n",
    "\n",
    "        std_with_outliers = column_data_with_outliers.std()\n",
    "        std_without_outliers = column_data_without_outliers.std()\n",
    "\n",
    "        std_difference = std_with_outliers - std_without_outliers\n",
    "\n",
    "        q75_with_outliers = column_data_with_outliers.quantile(0.75)\n",
    "        q75_without_outliers = column_data_without_outliers.quantile(0.75)\n",
    "\n",
    "        q75_difference = q75_with_outliers - q75_without_outliers\n",
    "\n",
    "        max_with_outliers = column_data_with_outliers.max()\n",
    "        min_with_outliers = column_data_with_outliers.min()\n",
    "\n",
    "        max_without_outliers = column_data_without_outliers.max()\n",
    "        min_without_outliers = column_data_without_outliers.min()\n",
    "\n",
    "        max_difference = max_with_outliers - max_without_outliers\n",
    "        min_difference = min_with_outliers - min_without_outliers\n",
    "\n",
    "        is_outlier = (abs(mean_difference) > significance_threshold) and \\\n",
    "                     (abs(median_difference) > significance_threshold) and \\\n",
    "                     (abs(std_difference) > significance_threshold) and \\\n",
    "                     (abs(q75_difference) > significance_threshold) and \\\n",
    "                     (abs(max_difference) > significance_threshold) and \\\n",
    "                     (abs(min_difference) > significance_threshold)\n",
    "\n",
    "        correction_approach = None\n",
    "\n",
    "        if is_outlier:\n",
    "            outlier_percentage = len(column_data_with_outliers) / len(column_data_without_outliers)\n",
    "            if outlier_percentage > removal_threshold:\n",
    "                correction_approach = 'remove_outliers'\n",
    "            else:\n",
    "                correction_approach = 'adjust_values'\n",
    "\n",
    "        return {'is_outlier': is_outlier, 'correction_approach': correction_approach}\n",
    "\n",
    "    # Detect outliers and decide on the approach for each column in each DataFrame\n",
    "    for df in data_frames:\n",
    "        for col in df.columns:\n",
    "            if col in measurement_columns_all:\n",
    "                if detect_outliers(df, col):\n",
    "                    outlier_info.setdefault(col, {'is_outlier': True, 'correction_approach': None})\n",
    "                else:\n",
    "                    outlier_info.setdefault(col, {'is_outlier': False, 'correction_approach': None})\n",
    "\n",
    "    return outlier_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99baf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_corrector_func(data_frames, outlier_info):\n",
    "    \"\"\"\n",
    "    Correct outliers in the given DataFrames based on the provided outlier information.\n",
    "\n",
    "    Args:\n",
    "    - dfs (list): List of DataFrames to be corrected.\n",
    "    - outlier_info (dict): Dictionary containing outlier information for each column.\n",
    "                           Keys: column name, Values: Dictionary with 'is_outlier' key indicating if it's an outlier.\n",
    "\n",
    "    Returns:\n",
    "    - corrected_dfs (list): List of corrected DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    corrected_dfs = []\n",
    "\n",
    "    for df in data_frames:\n",
    "        corrected_df = df.copy()  # Make a copy of the original DataFrame to avoid modifying it directly\n",
    "        for col, col_info in outlier_info.items():\n",
    "            if col_info['is_outlier']:\n",
    "                if col_info['correction_approach'] == 'remove_outliers':\n",
    "                    # Remove outliers from the column\n",
    "                    corrected_df = corrected_df[~(corrected_df[col].isin(df[col]))]\n",
    "                elif col_info['correction_approach'] == 'adjust_values':\n",
    "                    # Correct the outliers in the column\n",
    "                    # Replace outliers with the median of the column\n",
    "                    median_value = df[col].median()\n",
    "                    corrected_df[col] = df[col].apply(lambda x: median_value if col_info['is_outlier'] else x)\n",
    "        corrected_dfs.append(corrected_df)\n",
    "\n",
    "    return corrected_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e4418",
   "metadata": {},
   "source": [
    "<b>6. Handle Skewness and Kurtosis:</b> \n",
    "\n",
    "Are the values within a reasonable range for the context of the project?\n",
    "\n",
    "Is the data distribution skewed or heavily tailed?\n",
    "\n",
    "Does the data exhibit high kurtosis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e883f28",
   "metadata": {},
   "source": [
    "<b>Skewness:</b>\n",
    "\n",
    "Skewness measures the asymmetry of the distribution of values in a dataset.\n",
    "\n",
    "*For a perfectly symmetric distribution, the skewness should be close to 0.\n",
    "\n",
    "*Positive skewness indicates a longer right tail, so if you're aiming for symmetry, you'd want to reduce positive skewness towards 0.\n",
    "\n",
    "*Negative skewness indicates a longer left tail, so if you're aiming for symmetry, you'd want to reduce negative skewness towards 0.\n",
    "\n",
    "\n",
    "<b>Kurtosis:</b>\n",
    "\n",
    "*Kurtosis measures the tailedness or peakedness of the distribution of values in a dataset.\n",
    "\n",
    "*For a normal distribution, the kurtosis is 3.\n",
    "\n",
    "*Kurtosis greater than 3 indicates heavier tails and a more peaked distribution (leptokurtic).\n",
    "\n",
    "*Kurtosis less than 3 indicates lighter tails and a flatter distribution (platykurtic).\n",
    "\n",
    "*In the output, for example, the skewness value for the StepTotal column is approximately 4.832214, indicating that the distribution is right-skewed.. The kurtosis value for the StepTotal column is approximately 34.200632, indicating a heavily tailed distribution with many values in the tails.\n",
    "\n",
    "Based on these values, you can infer that the StepTotal column has a right-skewed distribution (positive skewness) and a heavily tailed distribution (high kurtosis). This information provides insights into the shape and characteristics of the distribution of values in the dataset.\n",
    "\n",
    "Transformations that can be applied to correct skewness and kurtosis:\n",
    "\n",
    "For Skewness:\n",
    "\n",
    "*Right-skewed data (positive skewness): square root, logarithm, or reciprocal to reduce the skewness towards 0.\n",
    "\n",
    "*Left-skewed data (negative skewness), you can apply transformations such as square or cube to reduce the skewness towards 0.\n",
    "\n",
    "For Kurtosis:\n",
    "\n",
    "*To reduce excessive kurtosis: square root, logarithm, or reciprocal to flatten the tails and reduce the peakiness of the distribution.\n",
    "\n",
    "*If the distribution is too flat(low kurtosis): squaring or cubing to increase kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3876a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewness_kurtosis_decider_func(dfs, measurement_column_all, significance_threshold=3):\n",
    "    \"\"\"\n",
    "    Decide whether columns in a list of DataFrames have significant skewness or kurtosis.\n",
    "    \n",
    "    Args:\n",
    "    - dfs (list): List of DataFrames containing the data.\n",
    "    - measurement_column_all (list): List of column names to process.\n",
    "    - significance_threshold (float): Threshold value to determine the significance of skewness or kurtosis.\n",
    "    \n",
    "    Returns:\n",
    "    - columns_with_significant_skewness (list): List of columns with significant skewness.\n",
    "    - columns_with_significant_kurtosis (list): List of columns with significant kurtosis.\n",
    "    - columns_without_significant_skewness (list): List of columns without significant skewness.\n",
    "    - columns_without_significant_kurtosis (list): List of columns without significant kurtosis.\n",
    "    \"\"\"\n",
    "    columns_with_significant_skewness = []\n",
    "    columns_with_significant_kurtosis = []\n",
    "    columns_without_significant_skewness = []\n",
    "    columns_without_significant_kurtosis = []\n",
    "    \n",
    "    for df in dfs:\n",
    "        if isinstance(df, pd.DataFrame):  # Check if df is a DataFrame\n",
    "            for col in measurement_column_all:\n",
    "                if col in df.columns:\n",
    "                    # Get the column data\n",
    "                    column_data = df[col]\n",
    "                    \n",
    "                    # Calculate skewness and kurtosis\n",
    "                    skewness = column_data.skew()\n",
    "                    kurtosis = column_data.kurtosis()\n",
    "                    \n",
    "                    # Determine significant skewness\n",
    "                    if abs(skewness) > significance_threshold:\n",
    "                        columns_with_significant_skewness.append((col, skewness))\n",
    "                    else:\n",
    "                        columns_without_significant_skewness.append((col, skewness))\n",
    "                    \n",
    "                    # Determine significant kurtosis\n",
    "                    if abs(kurtosis) > significance_threshold:\n",
    "                        columns_with_significant_kurtosis.append((col, kurtosis))\n",
    "                    else:\n",
    "                        columns_without_significant_kurtosis.append((col, kurtosis))\n",
    "                else:\n",
    "                    pass  # Column not found in the DataFrame\n",
    "        else:\n",
    "            print(\"Input is not a DataFrame.\")\n",
    "    \n",
    "    return (columns_with_significant_skewness, columns_with_significant_kurtosis,\n",
    "            columns_without_significant_skewness, columns_without_significant_kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dabcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_skewness_kurtosis_func(data_frames, columns_with_significant_skewness, columns_with_significant_kurtosis):\n",
    "    \"\"\"\n",
    "    Transforms the data frames to reduce skewness and kurtosis in specified columns.\n",
    "\n",
    "    Args:\n",
    "    - data_frames (list): List of pandas DataFrames to be transformed.\n",
    "    - columns_with_significant_skewness (list): List of column names with significant skewness.\n",
    "    - columns_with_significant_kurtosis (list): List of column names with significant kurtosis.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of transformed pandas DataFrames.\n",
    "    \"\"\"\n",
    "    transformed_dfs = []\n",
    "    for df in data_frames:\n",
    "        transformed_df = df.copy()\n",
    "        for col in df.columns:\n",
    "            if col in columns_with_significant_skewness:\n",
    "                # Plot histogram before skewness transformation\n",
    "                sns.histplot(df[col], bins=20, color='blue', alpha=0.7)\n",
    "                plt.title(f'Histogram of {col} before skewness transformation')\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.show()\n",
    "\n",
    "                # Perform skewness transformation\n",
    "                skewness = skew(df[col])\n",
    "                if skewness > 0:\n",
    "                    transformed_df[col] = np.log1p(df[col])\n",
    "                elif skewness < 0:\n",
    "                    transformed_df[col] = np.sqrt(df[col])\n",
    "\n",
    "                # Plot histogram after skewness transformation\n",
    "                sns.histplot(transformed_df[col], bins=20, color='red', alpha=0.7)\n",
    "                plt.title(f'Histogram of {col} after skewness transformation')\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.show()\n",
    "\n",
    "            if col in columns_with_significant_kurtosis:\n",
    "                # Plot histogram before kurtosis transformation\n",
    "                sns.histplot(df[col], bins=20, color='blue', alpha=0.7)\n",
    "                plt.title(f'Histogram of {col} before kurtosis transformation')\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.show()\n",
    "\n",
    "                # Perform kurtosis transformation\n",
    "                transformed_df[col] = boxcox(df[col])[0]\n",
    "\n",
    "                # Plot histogram after kurtosis transformation\n",
    "                sns.histplot(transformed_df[col], bins=20, color='red', alpha=0.7)\n",
    "                plt.title(f'Histogram of {col} after kurtosis transformation')\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.show()\n",
    "\n",
    "        transformed_dfs.append(transformed_df)\n",
    "    \n",
    "    return transformed_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf60f9",
   "metadata": {},
   "source": [
    "<b>7. Data encoding:</b>\n",
    "    \n",
    "Do boolean values need to be encoded to 1 for True and 0 for False?\n",
    "\n",
    "How will data encoding of boolean/or categorical values impact data interpretation and analysis?\n",
    "\n",
    "Do you need to add comments to indicate the meaning of True (1) and False (0) in the respective columns?\n",
    "\n",
    "I encoded the boolean values in the 'boolean_column' as integers, where True is represented as 1 and False as 0.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6017c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encoding_func(cleansed_data_frames_skewness_kurtosis, categorical_columns_all):\n",
    "    \"\"\"\n",
    "    Encode categorical columns in the given DataFrames.\n",
    "\n",
    "    Args:\n",
    "    - cleansed_data_frames_skewness_kurtosis (list): List of DataFrames to be encoded.\n",
    "    - categorical_columns_all (list): List of column names to encode as categorical.\n",
    "\n",
    "    Returns:\n",
    "    - encoded_data_frames (list): List of encoded DataFrames.\n",
    "    \"\"\"\n",
    "    encoded_data_frames = []\n",
    "\n",
    "    for df in cleansed_data_frames_skewness_kurtosis:\n",
    "        encoded_df = df.copy()  # Make a copy of the original DataFrame to avoid modifying it directly\n",
    "        for col in df.columns:\n",
    "            if col in categorical_columns_all:\n",
    "                if df[col].dtype == 'bool':\n",
    "                    # Convert boolean values to integer (0 or 1)\n",
    "                    encoded_df[col] = df[col].astype(int)\n",
    "                else:\n",
    "                    pass\n",
    "        encoded_data_frames.append(encoded_df)\n",
    "\n",
    "    return encoded_data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d28c21",
   "metadata": {},
   "source": [
    "<b>8. Datetime Decomposition:</b>\n",
    "\n",
    "This term describes the process of breaking down a datetime variable into its constituent parts, such as date and time components.\n",
    "\n",
    "Do datetime values need to be split into separate date and time columns?\n",
    "\n",
    "How will splitting datetime values impact subsequent analysis or modeling tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5031b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_decomposer_func(cleansed_data_frames_encoding, datetime_columns_all, date_columns_all):\n",
    "    \"\"\"\n",
    "    Decompose datetime columns into date, time, and period_of_day components.\n",
    "\n",
    "    Args:\n",
    "    - cleansed_data_frames_encoding (list): List of DataFrames to be decomposed.\n",
    "    - datetime_columns_all (list): List of column names representing datetime values.\n",
    "    - date_columns_all (list): List of column names representing date values.\n",
    "\n",
    "    Returns:\n",
    "    - decomposed_data_frames (list): List of DataFrames with decomposed datetime columns.\n",
    "    \"\"\"\n",
    "    decomposed_data_frames = []\n",
    "\n",
    "    for df in cleansed_data_frames_encoding:\n",
    "        decomposed_df = df.copy()  # Make a copy of the original DataFrame to avoid modifying it directly\n",
    "        for col in df.columns:\n",
    "            if col in datetime_columns_all:\n",
    "                # Decompose datetime column into date, time, and period_of_day components\n",
    "                decomposed_df[['date', 'time', 'period_of_day']] = df[col].astype(str).str.split(' ', expand=True)\n",
    "                decomposed_df['date'] = pd.to_datetime(decomposed_df['date'])\n",
    "                decomposed_df['time'] = pd.to_datetime(decomposed_df['time'], format='%H:%M:%S').dt.time\n",
    "            elif col in date_columns_all:\n",
    "                # Convert date column to datetime type\n",
    "                decomposed_df[col] = pd.to_datetime(df[col])\n",
    "            else:\n",
    "                pass  # Column not to be decomposed\n",
    "        decomposed_data_frames.append(decomposed_df)\n",
    "\n",
    "\n",
    "    return decomposed_data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e1062",
   "metadata": {},
   "source": [
    "<b>9. Row Identification:</b>\n",
    "\n",
    "This term describes the process of assigning a unique identifier to each row in a dataset.\n",
    "\n",
    "Is it necessary to add a unique identifier to each row in the dataset?\n",
    "\n",
    "How will the addition of a unique ID affect data manipulation and analysis?\n",
    "\n",
    "\"I performed row identification by adding a unique ID to each row in the DataFrame.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3354b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_id(cleansed_data_frames_datetime):\n",
    "    \"\"\"\n",
    "    Generate a unique ID based on the file name and auto-incrementing serial number for each row\n",
    "    and move the 'file_name' column to the last position in each DataFrame in the list.\n",
    "\n",
    "    Args:\n",
    "    - cleansed_data_frames_datetime (list): List of DataFrames to which unique IDs will be added.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of DataFrames with unique IDs added as indices for each row and 'file_name'\n",
    "    column moved to the last position.\n",
    "    \"\"\"\n",
    "    cleansed_data_frames_unique_id = []\n",
    "    \n",
    "    for df in cleansed_data_frames_datetime:\n",
    "        if 'file_name' in df.columns:\n",
    "            file_name = df['file_name'].iloc[0].split('.')[0]  # Extract file name\n",
    "            initials = ''.join(word[:2].upper() for word in file_name.split('_'))\n",
    "            serial_numbers = range(1, len(df) + 1)\n",
    "            unique_ids = [f'{initials}{i}' for i in serial_numbers]\n",
    "            df.insert(0, 'unique_id', unique_ids)  # Insert 'unique_id' column at the first position\n",
    "\n",
    "        # Rearrange columns to ensure 'file_name' is at the last position\n",
    "        if 'file_name' in df.columns:\n",
    "            columns = list(df.columns)  # Get list of current columns\n",
    "            columns.remove('file_name')  # Remove 'file_name' from the list\n",
    "            columns.append('file_name')  # Append 'file_name' to the end of the list\n",
    "            df = df[columns]  # Update DataFrame with the new column order\n",
    "            \n",
    "        cleansed_data_frames_unique_id.append(df)\n",
    "\n",
    "    \n",
    "    return cleansed_data_frames_unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5627179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transformation_func(extracted_datax):\n",
    "    \"\"\"\n",
    "    Perform data transformation pipeline on the extracted data.\n",
    "\n",
    "    Args:\n",
    "    - extracted_datax: Data extracted from the source.\n",
    "\n",
    "    Returns:\n",
    "    - transformed_data_frames: Transformed data frames ready for analysis or modeling.\n",
    "    \"\"\"\n",
    "    # Step 1: Clean Duplicates\n",
    "    cleansed_data_frames_duplicates = handle_duplicates(extracted_datax)\n",
    "\n",
    "    # Step 2: Clean Missing Values\n",
    "    cleansed_data_frames_missing_values = handle_missing_values(cleansed_data_frames_duplicates)\n",
    "\n",
    "    # Step 3: Classify Data Types\n",
    "    numeric_columns_all, datetime_columns_all, date_columns_all, categorical_columns_all, cleansed_data_frames_datatypes = classify_datatypes(cleansed_data_frames_missing_values)\n",
    "\n",
    "    # Step 4: Categorize Numeric Columns\n",
    "    count_columns_all, measurement_columns_all, cleansed_data_frames_classify_numbers = classify_numeric_columns(cleansed_data_frames_datatypes, numeric_columns_all)\n",
    "\n",
    "    # Step 5: Handle Outliers\n",
    "    outlier_info = outlier_detector_and_decider_func(cleansed_data_frames_classify_numbers, measurement_columns_all)\n",
    "    cleansed_data_frames_outlier = outlier_corrector_func(cleansed_data_frames_classify_numbers, outlier_info)\n",
    "\n",
    "    # Step 6: Handle Skewness and Kurtosis\n",
    "    columns_with_significant_skewness, columns_with_significant_kurtosis, columns_without_significant_skewness, columns_without_significant_kurtosis = skewness_kurtosis_decider_func(cleansed_data_frames_outlier, measurement_columns_all)\n",
    "    cleansed_data_frames_skewness_kurtosis = transform_skewness_kurtosis_func(cleansed_data_frames_outlier, columns_with_significant_skewness, columns_with_significant_kurtosis)\n",
    "\n",
    "    # Step 7: Encode Boolean Values\n",
    "    cleansed_data_frames_encoding = data_encoding_func(cleansed_data_frames_skewness_kurtosis, categorical_columns_all)\n",
    "\n",
    "    # Step 8: Decompose Datetime\n",
    "    cleansed_data_frames_datetime = datetime_decomposer_func(cleansed_data_frames_encoding, datetime_columns_all, date_columns_all)\n",
    "\n",
    "    # Step 9: Row Identification (if applicable)\n",
    "    cleansed_data_frames_row_identification = generate_unique_id(cleansed_data_frames_datetime)\n",
    "\n",
    "    # Step 10: Return Transformed Data Frames\n",
    "    transformed_data_frames = cleansed_data_frames_datetime\n",
    "\n",
    "    return transformed_data_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac983e45",
   "metadata": {},
   "source": [
    "# 10. Load data to Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45baea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformed_data_func(transformed_dataframe_list, DEST_USERNAME, DEST_PASSWORD, DEST_SERVER, DEST_DATABASE):\n",
    "    \"\"\"\n",
    "    Loads transformed data into the destination PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "    - transformed_dataframe_list (list): List of DataFrames containing transformed data.\n",
    "    - DEST_USERNAME (str): Username for the destination database connection.\n",
    "    - DEST_PASSWORD (str): Password for the destination database connection.\n",
    "    - DEST_SERVER (str): Server address for the destination database connection.\n",
    "    - DEST_DATABASE (str): Name of the destination database.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Destination database connection string\n",
    "    dest_conn_str = f'postgresql://{DEST_USERNAME}:{DEST_PASSWORD}@{DEST_SERVER}/{DEST_DATABASE}'\n",
    "\n",
    "    try:\n",
    "        # Establish connection to the destination database\n",
    "        dest_engine = create_engine(dest_conn_str)\n",
    "\n",
    "        # Add new columns 'fileimportedby' and 'fileimportdatetime' to each DataFrame\n",
    "        for df in transformed_dataframe_list:\n",
    "            df['fileimportedby'] = DEST_USERNAME\n",
    "            df['fileimportdatetime'] = datetime.now()\n",
    "\n",
    "        # Create a sessionmaker to handle transactions\n",
    "        Session = sessionmaker(bind=dest_engine)\n",
    "\n",
    "        # Loop through each DataFrame in transformed_dataframe_list\n",
    "        for transformed_data in transformed_dataframe_list:\n",
    "            # Extract the table name from the DataFrame\n",
    "            table_name = transformed_data['file_name'].iloc[0].split('.')[0]\n",
    "\n",
    "            # Check if the table already exists in the database\n",
    "            table_exists = dest_engine.has_table(table_name)\n",
    "\n",
    "            if not table_exists:\n",
    "                # If the table does not exist, create it\n",
    "                transformed_data.head(0).to_sql(table_name, dest_engine, index=False)\n",
    "\n",
    "            # Load transformed data into the destination database\n",
    "            with dest_engine.connect() as conn:\n",
    "                # Append transformed data to the existing table\n",
    "                transformed_data.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "\n",
    "        logger.info(\"Transformed data loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {str(e)}\")\n",
    "        raise  # Re-raise the exception for higher-level handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2143ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def fitbit_etl_pipeline():\n",
    "    \"\"\"\n",
    "    Executes the Fitbit ETL (Extract, Transform, Load) pipeline.\n",
    "\n",
    "    1. Input dataset folder path and database credentials.\n",
    "    2. Extracts data from the source dataset folder.\n",
    "    3. Extracts data from the source database and stages it for transformation.\n",
    "    4. Transforms the extracted data.\n",
    "    5. Loads the transformed data into the destination database.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    try:\n",
    "        # 1. Input dataset folder path and database credentials\n",
    "        logger.info(\"Step 1: Input dataset folder path and database credentials.\")\n",
    "        input_result = inputer_func()\n",
    "        folder_path, SOURCE_USERNAME, SOURCE_PASSWORD, SOURCE_SERVER, SOURCE_DATABASE, DEST_USERNAME, DEST_PASSWORD, DEST_SERVER, DEST_DATABASE = input_result\n",
    "        \n",
    "        # Validate input\n",
    "        if not all(input_result):\n",
    "            raise ValueError(\"Invalid input. Please provide all required parameters.\")\n",
    "\n",
    "        # 2. Sourcing Data phase\n",
    "        logger.info(\"Step 2: Data sourcing phase.\")\n",
    "        sourced_dataframe_list = source_data_func(folder_path)\n",
    "\n",
    "        # 3. Staging phase\n",
    "        logger.info(\"Step 3: Staging phase.\")\n",
    "        staged_dataframe_list = staging_func(sourced_dataframe_list, SOURCE_USERNAME, SOURCE_PASSWORD, SOURCE_SERVER, SOURCE_DATABASE)\n",
    "        \n",
    "        # 4. Extraction phase\n",
    "        logger.info(\"Step 4: Extraction phase.\")\n",
    "        extracted_data_list = extraction_func(SOURCE_USERNAME, SOURCE_PASSWORD, SOURCE_SERVER, SOURCE_DATABASE)\n",
    "\n",
    "        # 5. Transformation phase\n",
    "        logger.info(\"Step 5: Transformation phase.\")\n",
    "        transformed_dataframe_list = transformation_func(extracted_data_list)\n",
    "\n",
    "        # 6. Loading phase\n",
    "        logger.info(\"Step 6: Loading phase.\")\n",
    "        load_transformed_data_func(transformed_dataframe_list, DEST_USERNAME, DEST_PASSWORD, DEST_SERVER, DEST_DATABASE)\n",
    "        \n",
    "        logger.info(\"ETL process completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during ETL process: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "935c4988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Step 1: Input dataset folder path and database credentials.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the folder path (without quotes): C:\\Users\\admin\\Desktop\\Projects\\Project FitBit\\Fitabase Dataset\n",
      "Enter Source Username: fitbase\n",
      "Enter Source Password: fitbase\n",
      "Enter Source Server: localhost\n",
      "Enter Source Database: projectfitbase\n",
      "Enter Destination Username: fitbase\n",
      "Enter Destination Password: fitbase\n",
      "Enter Destination Server: localhost\n",
      "Enter Destination Database: projectfitbasetransformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Step 2: Data sourcing phase.\n",
      "INFO:__main__:Step 3: Staging phase.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging data for transformation...1709083421.5229754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Step 4: Extraction phase.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully staged. Time taken: 172.3397753238678 seconds\n",
      "Loading data from source database...1709083593.8627508 seconds\n",
      "Extracting data for transformation...1709083593.8627508 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Step 5: Transformation phase.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully extracted. Time taken: 139.68007040023804 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Step 6: Loading phase.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14620\\169144665.py:40: SADeprecationWarning: The Engine.has_table() method is deprecated and will be removed in a future release.  Please refer to Inspector.has_table(). (deprecated since: 1.4)\n",
      "  table_exists = dest_engine.has_table(table_name)\n",
      "INFO:__main__:Transformed data loaded successfully.\n",
      "INFO:__main__:ETL process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "fitbit_etl_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb7151db",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1150518328.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Enter the folder path: \"C:\\Users\\admin\\Desktop\\Projects\\Project FitBit\\Fitabase Dataset\"\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Enter the folder path: \"C:\\Users\\admin\\Desktop\\Projects\\Project FitBit\\Fitabase Dataset\"\n",
    "Enter Source Username: fitbase\n",
    "Enter Source Password: fitbase\n",
    "Enter Source Server: localhost\n",
    "Enter Source Database: projectfitbase\n",
    "Enter Destination Username: fitbase\n",
    "Enter Destination Password: fitbase\n",
    "Enter Destination Server: localhost\n",
    "Enter Destination Database: projectfitbasetransformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42222607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
